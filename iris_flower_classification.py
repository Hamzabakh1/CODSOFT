# -*- coding: utf-8 -*-
"""IRIS FLOWER CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y6LCIGRPAzyw3TrQ9RRk6pj8FreUcp_e
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, chi2

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IRIS.csv')

# Perform exploratory data analysis
sns.pairplot(df, hue='species')

# Preprocess the data
X = df.drop('species', axis=1)
y = df['species']

# Encode the categorical target variable to numerical
le = LabelEncoder()
y = le.fit_transform(y)

# Split the dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the models
models = {
    'knn': KNeighborsClassifier(),
    'svm': SVC(),
    'decision_tree': DecisionTreeClassifier(),
    'random_forest': RandomForestClassifier()
}

# Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")

    # Create a pipeline for each model
    pipe = Pipeline([('feature_selection', SelectKBest(score_func=chi2, k=3)),
                 ('scaler', StandardScaler()),
                 ('model', model)])

    # Train the model using the pipeline
    pipe.fit(X_train, y_train)

    print(f"Evaluating {name}...")
    y_pred = pipe.predict(X_test)
    print(classification_report(y_test, y_pred))

    # Calculate feature importance for tree-based models
if name in ['decision_tree', 'random_forest']:
    result = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=42)
    perm_sorted_idx = result.importances_mean.argsort()
    tree_importance_sorted_idx = np.argsort(pipe.named_steps['model'].feature_importances_)
    tree_indices = np.arange(0, len(pipe.named_steps['model'].feature_importances_)) + 0.5

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))
    ax1.barh(tree_indices, pipe.named_steps['model'].feature_importances_[tree_importance_sorted_idx], height=0.7)
    ax1.set_yticks(tree_indices)  # Set yticks here
    ax1.set_yticklabels(df.columns[tree_importance_sorted_idx])
    ax1.set_ylim((0, len(pipe.named_steps['model'].feature_importances_)))
    ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False, labels=df.columns[perm_sorted_idx])
    fig.tight_layout()
    plt.show()